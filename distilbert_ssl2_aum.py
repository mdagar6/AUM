# -*- coding: utf-8 -*-
"""distilbert_SSL2_AUM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nb2NsSay5I_KATQ8X24_33w5BzwbeL__

# Package Install
"""

!pip install aum
!pip install transformers
!pip install nlp
!pip install datasets


"""# Import"""

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
import torch
import pandas as pd
from datasets import Dataset
from aum import AUMCalculator
import random
from tqdm.notebook import tqdm
from  matplotlib import pyplot
import seaborn
import os
import re
import argparse

"""# Download and read Dataset"""

!rm *.zip
!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip
!unzip SST-2.zip

train = Dataset.from_pandas(pd.read_csv("/content/SST-2/train.tsv",sep='\t',header='infer',index_col=0))
test = Dataset.from_pandas(pd.read_csv("/content/SST-2/test.tsv",sep='\t',header='infer',index_col=0))
dev = Dataset.from_pandas(pd.read_csv("/content/SST-2/dev.tsv",sep='\t',header='infer',index_col=0))

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Tokenize sentence
train_enc = tokenizer(train['sentence'], truncation=True, padding=True,max_length = 128)
test_enc = tokenizer(test['sentence'], truncation=True, padding=True,max_length = 128)
dev_enc = tokenizer(dev['sentence'], truncation=True, padding=True,max_length = 128)

"""# With Original DataSet (No AUM calculation)

## Dataset Preparation
"""

# Training data preparation
class SST(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SST(train_enc, (torch.Tensor(train['label'])).long())
dev_dataset = SST(dev_enc, (torch.Tensor(dev['label'])).long())

"""## Training"""

def training(model, bsize, n_epochs, train_data, lrate, optimizer, with_aum):
  dataloader = torch.utils.data.DataLoader(train_data, batch_size=bsize, shuffle=True)
  model.cuda()
  opt = optimizer(params = model.parameters(),lr=lrate)
  model.train()
  for i in tqdm(range(n_epochs)):
    total_train_loss = 0.0
    acc = 0.0
    for batch in tqdm(dataloader, leave=False):
      label = batch["labels"].cuda()
      data = batch["input_ids"].cuda()
      attention_mask = batch["attention_mask"].cuda()

      outputs = model(data, attention_mask=attention_mask, labels=label)
      acc += torch.sum(outputs.logits.argmax(dim=1)==label)
      loss = outputs.loss
      
      if with_aum:
        index = batch["index"].cuda()
        records = aum_calculator.update(outputs.logits, label, index)
      
      opt.zero_grad()
      loss.backward()
      opt.step()

      total_train_loss += loss

    print("Train_Loss:",total_train_loss / len(dataloader),"Acc: ", acc/len(train_data))
  
  if with_aum:
    if os.path.exists('/content/AUM/aum_values.csv'):
      os.remove('/content/AUM/aum_values.csv')
    aum_calculator.finalize()
  
  return model

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
BATCH_SIZE = 32
N_EPOCHS = 3
lr=1e-5
optimizer = torch.optim.Adam
model = training(model, BATCH_SIZE, N_EPOCHS, train_dataset, lr, optimizer, False)

#SAVE model
parser = argparse.ArgumentParser()
parser.add_argument('--checkpoint-dir', type=str)
args = parser.parse_args()
os.makedirs(args.checkpoint_dir, exist_ok=True)
torch.save(net.state_dict(), os.path.join(args.checkpoint_dir, "model_without_AUM.pth"))

"""## Evaluation"""

def evaluation(model, test_data, bsize):
  test_loader = torch.utils.data.DataLoader(test_data, batch_size=bsize, shuffle=True)
  model.eval()
  acc = 0.0
  for batch in test_loader:
    input_ids = batch['input_ids'].cuda()
    attention_mask = batch['attention_mask'].cuda()
    labels = batch['labels'].cuda()
    outputs = model(input_ids, attention_mask, labels=labels)
    acc += torch.sum(outputs.logits.argmax(dim=-1)==labels)
  print("Test Acc: ", acc/len(test_data))

evaluation(model, dev_dataset, BATCH_SIZE)

"""# AUM Calculation

## Dataset Preparation 1
"""

num_classes = 2
fake_label = 2
N = len(train)
fakedata_count = int(N/(num_classes+1))

random_index = random.sample(range(N), fakedata_count)

# Add fake data and index
class SST_AUM(torch.utils.data.Dataset):
    def __init__(self, encodings, labels, random_index):
        self.random_index = random_index
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if idx in self.random_index:
            item['labels'] = torch.tensor(fake_label)
        else:
            item['labels'] = torch.tensor(self.labels[idx])
        item['index'] = idx
        return item

    def __len__(self):
        return len(self.labels)

train_dataset_aum = SST_AUM(train_enc, (torch.Tensor(train['label'])).long(), random_index)

"""## Training 1"""

save_dir = '/content/AUM'
aum_calculator = AUMCalculator(save_dir, compressed=True)

# training
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=(num_classes+1))
BATCH_SIZE = 32
N_EPOCHS = 3
lr=1e-5
optimizer = torch.optim.Adam
model = training(model, BATCH_SIZE, N_EPOCHS, train_dataset_aum, lr, optimizer, True)

"""## Calculate Threshold 1"""

def fetch_id(str):
  match = re.search(r'\d+', str)
  if match:
    return int(match.group(0))
  else:
    return None

# also add flag for fake data
def csv_to_dataframe(path, fake_data_index):
  aum = pd.read_csv(path,sep=',',header='infer')
  aum['aum'] = aum['aum'].apply(float) #df['a'] = df['a'].apply(lambda x: x + 1)
  aum['sample_id'] = aum['sample_id'].apply(fetch_id)
  aum = aum.groupby(['sample_id'],as_index=False).mean()
  # create flag for fake data
  fake_data_flag = [1 if i in fake_data_index else 0 for i in range(len(aum))] # this takes alot of time
  aum["fake_data_flag"] = fake_data_flag
  return aum

def plot_aum(aum_data):
  seaborn.set(style='ticks')
  fake= [0, 1]
  fg = seaborn.FacetGrid(data=aum_data, hue='fake_data_flag', hue_order=fake, aspect=1.61)
  fg.map(pyplot.scatter, 'aum', 'sample_id').add_legend()

def calculate_threshold(aum_data,percentile):
  return aum_data[aum_data['fake_data_flag'] == 1].aum.quantile(percentile)

aum_data = csv_to_dataframe("/content/AUM/aum_values.csv", random_index)

plot_aum(aum_data)

#99th percentile
threshold = calculate_threshold(aum_data, 0.7)

"""## Filter Data 1"""

filter_data1 = list(aum_data[(aum_data["fake_data_flag"] == 0) & (aum_data['aum'] < threshold)]["sample_id"])

len(filter_data1)

"""## Dataset Preparation 2"""

# remaining data
temp = set(range(N)) - set(random_index)
random_index_2 = random.sample(temp, fakedata_count)
train_dataset_aum_2 = SST_AUM(train_enc, (torch.Tensor(train['label'])).long(), random_index_2)

"""## Training 2 (To find mislabed in altered data)"""

# training
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=(num_classes+1))
BATCH_SIZE = 32
N_EPOCHS = 3
lr=1e-5
optimizer = torch.optim.Adam
model = training(model, BATCH_SIZE, N_EPOCHS, train_dataset_aum_2, lr, optimizer, True)

"""## threshold calculation 2"""

aum_data = csv_to_dataframe("/content/AUM/aum_values.csv", random_index_2)

plot_aum(aum_data)
threshold = calculate_threshold(aum_data, 0.1)

"""## Final Data Filter"""

temp = list(aum_data[(aum_data["fake_data_flag"] == 0) & (aum_data['aum'] < threshold)]["sample_id"])

mislabelled_data = set(filter_data1).union(set(temp))

print("Mislabelled_data from thershold set 1: ", len(filter_data1))
print("Mislabelled_data from thershold set 2: ", len(temp))
print("Mislabelled_data: ", len(mislabelled_data))

"""# Train on filter data

## Dataset Preparation
"""

#mislabelled_data = list(mislabelled_data)

filter_index = list(set(range(N)) - set(mislabelled_data))
train_dataset_clean = SST(train_enc, (torch.Tensor(train['label'])).long())
train_dataset_clean = torch.utils.data.Subset(train_dataset_clean, filter_index)



"""## Training"""

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
BATCH_SIZE = 32
N_EPOCHS = 3
lr=1e-5
optimizer = torch.optim.Adam
model = training(model, BATCH_SIZE, N_EPOCHS, train_dataset_clean, lr, optimizer, False)

"""## Evaluation"""

evaluation(model, dev_dataset, BATCH_SIZE)
