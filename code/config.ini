[distilBERT_SST2]
# Execution details
checkpoint_dir = /scratch/md4676/NLU/checkpoint/
training_epoch = 3
batch_size = 32
learning_rate = 1e-5
optimizer = torch.optim.Adam

# Architecture
model = distilbert
#100,6,64
random_seed = 100

# Dataset
dataset = SST2
dataset_dir = /scratch/md4676/NLU/dataset/
data_file_type = .tsv
data_file_sep = \t
#0.00,0.20,0.40,0.60
synthetic_noise = 0.00

# AUM
aum_dir = /scratch/md4676/NLU/AUM/
fake_label = 2
num_classes = 2
threshold_percentile = 0.99
aum_training_epoch = 6
